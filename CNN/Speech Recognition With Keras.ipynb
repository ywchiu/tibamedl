{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梅爾倒頻譜\n",
    "在語音識別（Speech Recognition）和話者識別（Speaker Recognition）方面，最常用到的語音特徵就是梅爾倒譜係數（Mel-scale Frequency Cepstral Coefficients，簡稱MFCC）。根據人耳聽覺機理的研究發現，人耳對不同頻率的聲波有不同的聽覺敏感度。從200Hz到5000Hz的語音信號對語音的清晰度影響對大。\n",
    "\n",
    "兩個響度不等的聲音作用於人耳時，則響度較高的頻率成分的存在會影響到對響度較低的頻率成分的感受，使其變得不易察覺，這種現象稱為掩蔽效應。由於頻率較低的聲音在內耳蝸基底膜上行波傳遞的距離大於頻率較高的聲音，故一般來說，低音容易掩蔽高音，而高音掩蔽低音較困難。在低頻處的聲音掩蔽的臨界帶寬較高頻要小。所以，人們從低頻到高頻這一段頻帶內按臨界帶寬的大小由密到疏安排一組帶通濾波器，對輸入信號進行濾波。將每個帶通濾波器輸出的信號能量作為信號的基本特徵，對此特徵經過進一步處理後就可以作為語音的輸入特徵。\n",
    "\n",
    "由於這種特徵不依賴於信號的性質，對輸入信號不做任何的假設和限制，又利用了聽覺模型的研究成果。因此，這種參數比基於聲道模型的LPCC相比具有更好的Robustness，更符合人耳的聽覺特性，而且當信噪比降低時仍然具有較好的識別性能。\n",
    "\n",
    "梅爾倒頻譜係數通常是用以下方法得到的：\n",
    "\n",
    "- 將一訊號進行傅利葉轉換（Fourier transform）\n",
    "- 將頻譜映射（mapping）至梅爾刻度，利用三角窗函數（triangular overlapping window）\n",
    "- 取對數（logarithm）\n",
    "- 取離散餘弦轉換（discrete cosine transform）\n",
    "- MFCC是轉換後的頻譜\n",
    "\n",
    "\n",
    "參考資料：\n",
    "- http://mirlab.org/jang/books/audiosignalprocessing/speechFeatureMfcc_chinese.asp?title=12-2%20MFCC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料下載\n",
    "- https://drive.google.com/file/d/1QszsEG14awC5DwEWGIH0FGUobe7VtMnf/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 安裝librosa\n",
    "- pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa\n",
      "  Downloading https://files.pythonhosted.org/packages/09/b4/5b411f19de48f8fc1a0ff615555aa9124952e4156e94d4803377e50cfa4c/librosa-0.6.2.tar.gz (1.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.6MB 868kB/s ta 0:00:011\n",
      "\u001b[?25hCollecting audioread>=2.0.0 (from librosa)\n",
      "  Downloading https://files.pythonhosted.org/packages/f0/41/8cd160c6b2046b997d571a744a7f398f39e954a62dd747b2aae1ad7f07d4/audioread-2.1.6.tar.gz\n",
      "Requirement already satisfied: numpy>=1.8.0 in /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages (from librosa)\n",
      "Requirement already satisfied: scipy>=0.14.0 in /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages (from librosa)\n",
      "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages (from librosa)\n",
      "Collecting joblib>=0.12 (from librosa)\n",
      "  Downloading https://files.pythonhosted.org/packages/0d/1b/995167f6c66848d4eb7eabc386aebe07a1571b397629b2eac3b7bebdc343/joblib-0.13.0-py2.py3-none-any.whl (276kB)\n",
      "\u001b[K    100% |████████████████████████████████| 276kB 776kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: decorator>=3.0.0 in /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages (from librosa)\n",
      "Requirement already satisfied: six>=1.3 in /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages (from librosa)\n",
      "Collecting resampy>=0.2.0 (from librosa)\n",
      "  Downloading https://files.pythonhosted.org/packages/14/b6/66a06d85474190b50aee1a6c09cdc95bb405ac47338b27e9b21409da1760/resampy-0.2.1.tar.gz (322kB)\n",
      "\u001b[K    100% |████████████████████████████████| 327kB 1.8MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting numba>=0.38.0 (from librosa)\n",
      "  Downloading https://files.pythonhosted.org/packages/05/2b/e8a89afff91d29cac0020c2a6f79433c72b4bb0698b092e5cbc13718b51b/numba-0.41.0-cp36-cp36m-macosx_10_9_x86_64.whl (1.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.6MB 720kB/s ta 0:00:011\n",
      "\u001b[?25hCollecting llvmlite>=0.26.0dev0 (from numba>=0.38.0->librosa)\n",
      "  Downloading https://files.pythonhosted.org/packages/d7/05/be0997c6307063d3c8c788391a00b20a56a1b35c6859443383e12c85b061/llvmlite-0.26.0-cp36-cp36m-macosx_10_9_x86_64.whl (12.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 12.5MB 126kB/s ta 0:00:011\n",
      "\u001b[?25hBuilding wheels for collected packages: librosa, audioread, resampy\n",
      "  Running setup.py bdist_wheel for librosa ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/davidchiu/Library/Caches/pip/wheels/18/b8/10/f0f8f6ac60668a5cd75596cf14c25bb6b3ea1ecd815f058b7e\n",
      "  Running setup.py bdist_wheel for audioread ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/davidchiu/Library/Caches/pip/wheels/53/02/90/7b5c4081b7470c550ab605f600bad237dde12a6b8999b11f50\n",
      "  Running setup.py bdist_wheel for resampy ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/davidchiu/Library/Caches/pip/wheels/ff/4f/ed/2e6c676c23efe5394bb40ade50662e90eb46e29b48324c5f9b\n",
      "Successfully built librosa audioread resampy\n",
      "Installing collected packages: audioread, joblib, llvmlite, numba, resampy, librosa\n",
      "Successfully installed audioread-2.1.6 joblib-0.13.0 librosa-0.6.2 llvmlite-0.26.0 numba-0.41.0 resampy-0.2.1\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: Folder Path\n",
    "# Output: Tuple (Label, Indices of the labels, one-hot encoded labels)\n",
    "def get_labels(path=DATA_PATH):\n",
    "    labels = os.listdir(path)\n",
    "    label_indices = np.arange(0, len(labels))\n",
    "    return labels, label_indices, to_categorical(label_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handy function to convert wav2mfcc\n",
    "def wav2mfcc(file_path, max_len=11):\n",
    "    wave, sr = librosa.load(file_path, mono=True, sr=None)\n",
    "    wave = wave[::3]\n",
    "    mfcc = librosa.feature.mfcc(wave, sr=16000)\n",
    "\n",
    "    # If maximum length exceeds mfcc lengths then pad the remaining ones\n",
    "    if (max_len > mfcc.shape[1]):\n",
    "        pad_width = max_len - mfcc.shape[1]\n",
    "        mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "\n",
    "    # Else cutoff the remaining parts\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_len]\n",
    "    \n",
    "    return mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 從目錄夾讀取所有音檔，並將向量根據類別名稱存成 .npy 檔\n",
    "def save_data_to_array(path=DATA_PATH, max_len=11):\n",
    "    labels, _, _ = get_labels(path)\n",
    "\n",
    "    for label in labels:\n",
    "        # Init mfcc vectors\n",
    "        mfcc_vectors = []\n",
    "\n",
    "        wavfiles = [path + label + '/' + wavfile for wavfile in os.listdir(path + '/' + label)]\n",
    "        for wavfile in tqdm(wavfiles, \"Saving vectors of label - '{}'\".format(label)):\n",
    "            mfcc = wav2mfcc(wavfile, max_len=max_len)\n",
    "            mfcc_vectors.append(mfcc)\n",
    "        np.save(label + '.npy', mfcc_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 準備訓練與測試資料集\n",
    "- https://drive.google.com/file/d/1hThrCSQ5D8LNlVvkdqPbop1uTTsfl8lL/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(split_ratio=0.6, random_state=42):\n",
    "    # Get available labels\n",
    "    labels, indices, _ = get_labels(DATA_PATH)\n",
    "    print(labels)\n",
    "    # Getting first arrays\n",
    "    X = np.load(DATA_PATH + labels[0] )\n",
    "    y = np.zeros(X.shape[0])\n",
    "\n",
    "    # Append all of the dataset into one single array, same goes for y\n",
    "    for i, label in enumerate(labels[1:]):\n",
    "        x = np.load(DATA_PATH + label )\n",
    "        X = np.vstack((X, x))\n",
    "        y = np.append(y, np.full(x.shape[0], fill_value= (i + 1)))\n",
    "\n",
    "    assert X.shape[0] == len(y)\n",
    "\n",
    "    return train_test_split(X, y, test_size= (1 - split_ratio), random_state=random_state, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(path=DATA_PATH):\n",
    "    labels, _, _ = get_labels(path)\n",
    "    data = {}\n",
    "    for label in labels:\n",
    "        data[label] = {}\n",
    "        data[label]['path'] = [path  + label + '/' + wavfile for wavfile in os.listdir(path + '/' + label)]\n",
    "\n",
    "        vectors = []\n",
    "\n",
    "        for wavfile in data[label]['path']:\n",
    "            wave, sr = librosa.load(wavfile, mono=True, sr=None)\n",
    "            # Downsampling\n",
    "            wave = wave[::3]\n",
    "            mfcc = librosa.feature.mfcc(wave, sr=16000)\n",
    "            vectors.append(mfcc)\n",
    "\n",
    "        data[label]['mfcc'] = vectors\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path=DATA_PATH):\n",
    "    data = prepare_dataset(path)\n",
    "\n",
    "    dataset = []\n",
    "\n",
    "    for key in data:\n",
    "        for mfcc in data[key]['mfcc']:\n",
    "            dataset.append((key, mfcc))\n",
    "\n",
    "    return dataset[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/davidchiu/course/tibamedl/CNN'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat.npy', 'happy.npy', 'bed.npy']\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "DATA_PATH = \"./data/\"\n",
    "# Second dimension of the feature is dim2\n",
    "feature_dim_2 = 11\n",
    "\n",
    "# Save data to array file first\n",
    "# save_data_to_array(max_len=feature_dim_2)\n",
    "\n",
    "# # Loading train set and test set\n",
    "X_train, X_test, y_train, y_test = get_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3112, 20, 11, 1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Feature dimension\n",
    "feature_dim_1 = 20\n",
    "channel = 1\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "num_classes = 3\n",
    "\n",
    "# Reshaping to perform 2D convolution\n",
    "X_train = X_train.reshape(X_train.shape[0], feature_dim_1, feature_dim_2, channel)\n",
    "X_test = X_test.reshape(X_test.shape[0], feature_dim_1, feature_dim_2, channel)\n",
    "\n",
    "y_train_hot = to_categorical(y_train)\n",
    "y_test_hot = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# convolution\n",
    "model.add(Conv2D(32, kernel_size=(2, 2), activation='relu', input_shape=(feature_dim_1, feature_dim_2, channel)))\n",
    "model.add(Conv2D(48, kernel_size=(2, 2), activation='relu'))\n",
    "model.add(Conv2D(120, kernel_size=(2, 2), activation='relu'))\n",
    "\n",
    "# mat pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# flattening\n",
    "model.add(Flatten())\n",
    "\n",
    "# fully connected\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3112 samples, validate on 2076 samples\n",
      "Epoch 1/10\n",
      "3112/3112 [==============================] - 2s 690us/step - loss: 1.7833 - acc: 0.4547 - val_loss: 0.7200 - val_acc: 0.7563\n",
      "Epoch 2/10\n",
      "3112/3112 [==============================] - 2s 512us/step - loss: 0.6923 - acc: 0.7105 - val_loss: 0.5162 - val_acc: 0.8068\n",
      "Epoch 3/10\n",
      "3112/3112 [==============================] - 2s 560us/step - loss: 0.5226 - acc: 0.7982 - val_loss: 0.4106 - val_acc: 0.8603\n",
      "Epoch 4/10\n",
      "3112/3112 [==============================] - 2s 539us/step - loss: 0.4251 - acc: 0.8454 - val_loss: 0.3326 - val_acc: 0.8844\n",
      "Epoch 5/10\n",
      "3112/3112 [==============================] - 2s 565us/step - loss: 0.3503 - acc: 0.8747 - val_loss: 0.3336 - val_acc: 0.8829\n",
      "Epoch 6/10\n",
      "3112/3112 [==============================] - 2s 581us/step - loss: 0.3297 - acc: 0.8850 - val_loss: 0.2785 - val_acc: 0.8979\n",
      "Epoch 7/10\n",
      "3112/3112 [==============================] - 2s 618us/step - loss: 0.2642 - acc: 0.9042 - val_loss: 0.2562 - val_acc: 0.9162\n",
      "Epoch 8/10\n",
      "3112/3112 [==============================] - 2s 638us/step - loss: 0.2201 - acc: 0.9187 - val_loss: 0.2549 - val_acc: 0.9118\n",
      "Epoch 9/10\n",
      "3112/3112 [==============================] - 2s 576us/step - loss: 0.2073 - acc: 0.9242 - val_loss: 0.2373 - val_acc: 0.9181\n",
      "Epoch 10/10\n",
      "3112/3112 [==============================] - 2s 573us/step - loss: 0.1899 - acc: 0.9316 - val_loss: 0.2420 - val_acc: 0.9181\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12354a668>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train_hot, \n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs, \n",
    "          verbose=1, \n",
    "          validation_data=(X_test, y_test_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicts one sample\n",
    "def predict(filepath, model):\n",
    "    sample = wav2mfcc(filepath)\n",
    "    sample_reshaped = sample.reshape(1, feature_dim_1, feature_dim_2, channel)\n",
    "    return get_labels()[0][\n",
    "            np.argmax(model.predict(sample_reshaped))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happy.npy'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('3bdf05d3_nohash_0.wav', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bed.npy'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('7081436f_nohash_1.wav', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy 讀取與寫入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(10)\n",
    "np.save('my_array', x)\n",
    "np.load('my_array.npy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
