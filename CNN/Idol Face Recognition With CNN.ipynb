{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下載檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://raw.githubusercontent.com/ywchiu/tibamedl/master/Data/zhiyu_face.zip\n",
    "! wget https://raw.githubusercontent.com/ywchiu/tibamedl/master/Data/aragaki_face.zip\n",
    "! wget https://raw.githubusercontent.com/ywchiu/tibamedl/master/Data/puff_face.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解壓縮檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! unzip zhiyu_face.zip\n",
    "! unzip puff_face.zip\n",
    "! unzip aragaki_face.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立訓練與測試資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf train\n",
    "! rm -rf test\n",
    "\n",
    "import os\n",
    "\n",
    "if not os.path.exists('train/'):\n",
    "    os.mkdir('train/')\n",
    "\n",
    "if not os.path.exists('test/'):\n",
    "    os.mkdir('test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def splitDataset(dirs):\n",
    "    dataset = list(os.listdir(dirs))\n",
    "    train_data, test_data = train_test_split(dataset, test_size= 0.2, random_state = 42)\n",
    "    \n",
    "    if not os.path.exists('train/'+dirs):\n",
    "        os.mkdir('train/'+dirs)\n",
    "\n",
    "    if not os.path.exists('test/'+dirs):\n",
    "        os.mkdir('test/'+dirs)\n",
    "\n",
    "    for f in train_data:\n",
    "        os.rename(dirs + f, 'train/'+ dirs + f)\n",
    "\n",
    "    for f in test_data:\n",
    "        os.rename(dirs + f, 'test/' + dirs +f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitDataset('zhiyu_face/')\n",
    "splitDataset('puff_face/')\n",
    "splitDataset('aragaki_face/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立 CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten,Dropout\n",
    "# Initialising the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Convolution\n",
    "classifier.add(Conv2D(filters=32, \n",
    "           kernel_size=(3, 3), \n",
    "           padding = 'same',\n",
    "           input_shape = (64, 64, 3),\n",
    "           activation = 'relu'))\n",
    "\n",
    "# Max Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Convolution\n",
    "classifier.add(Conv2D(filters=32, \n",
    "          kernel_size=(3, 3), \n",
    "          padding = 'same', \n",
    "          activation = 'relu'))\n",
    "\n",
    "# Max Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Fully Connected\n",
    "classifier.add(Dense(units = 128, activation = 'relu')) \n",
    "classifier.add(Dropout(rate=0.1))  \n",
    "classifier.add(Dense(units = 3, activation = 'softmax'))\n",
    "\n",
    "# Compile\n",
    "classifier.compile(optimizer = 'adam', \n",
    "          loss ='categorical_crossentropy', \n",
    "          metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料增幅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    'train/', target_size = (64, 64),\n",
    "     batch_size = 32,\n",
    "     shuffle=True,\n",
    "     class_mode = 'categorical')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    'test/', target_size = (64, 64),\n",
    "    class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = classifier.fit(training_set,\n",
    "            epochs=10,\n",
    "            verbose = 1,\n",
    "            validation_data = test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 單張預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "res = requests.get('https://upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Chou_Tzuyu_at_the_Golden_Disc_Awards_2019.png/220px-Chou_Tzuyu_at_the_Golden_Disc_Awards_2019.png')\n",
    "with open('zhiyu_test.png', 'wb') as f:\n",
    "    f.write(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "from PIL import Image\n",
    "\n",
    "image = face_recognition.load_image_file('zhiyu_test.png')\n",
    "face_locations = face_recognition.face_locations(image)\n",
    "top, right, bottom, left = face_locations[0]\n",
    "faceImage = image[top:bottom, left:right]\n",
    "final = Image.fromarray(faceImage)\n",
    "crpim   = final.resize((64,64))\n",
    "crpim.save('predict_face.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crpim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('predict_face.png', target_size= (64,64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "classifier.predict_classes(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 遷移學習 (Transfer Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install keras_vggface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_vggface.vggface import VGGFace\n",
    "\n",
    "\n",
    "vgg_model = VGGFace(include_top=False, input_shape=(64, 64, 3), pooling='avg')\n",
    "vgg_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Initialising the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Add VGG16\n",
    "classifier.add(vgg_model)\n",
    "\n",
    "# Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Fully Connected\n",
    "classifier.add(Dense(units = 512, activation = 'relu')) \n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Dropout(rate=0.1))  \n",
    "\n",
    "classifier.add(Dense(units = 512, activation = 'relu')) \n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Dropout(rate=0.1))  \n",
    "\n",
    "classifier.add(Dense(units = 3, activation = 'softmax'))\n",
    "\n",
    "classifier.compile(optimizer = 'adam', \n",
    "          loss ='categorical_crossentropy', \n",
    "          metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = classifier.fit(training_set,\n",
    "            epochs=100,\n",
    "            verbose = 1,\n",
    "            validation_data = test_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
